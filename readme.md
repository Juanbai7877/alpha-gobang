# alpha-gobang

## requirement

```
python>=3.12.2
torch>=2.2.2
```
注：更低版本的未作尝试，不代表不能运行。

## 细节

### 怎么开始

```
python gobang_train.py
```

### 训练方法

* 使用强化学习策略
  * `agent` \ `environment` \ `action` \ `reward` \ `state`
  * `agent`在特定的`environment`（以`state`量化）作出`action`可以得到对应的`reward`
* 使用`类Q-learning`策略
  * 对于当前某一位置的预测值，其应该是当前位置的`reward`+`未来可能得到的最大分数`
    * 本问题特殊在：
      * `agent`在一次`action`后需要另一个`agent`来进行一次`action`，所以`未来可能得到的最大分数`在本问题中被认为是**对方的最大的负数**（博弈论）
      * `reward`难设置,目前将其设置为`environment.gobang.game.get_reward`
        * 占空格得分：对一个位置上的对角线、竖线、横线上的连续空格统计，每个计入`10`分。
        * 连续放置得分：对一个位置上的对角线、竖线、横线上的连续同色统计，每个计入`50`分。
        * 防守得分:对一个位置上的对角线、竖线、横线上的连续它色统计，每个计入`100`分。
        * 顺利得分:`2560`分.
        * [未加入]重复放置扣分
* 怎么组织训练?
  * 目前是分为`A`与`B`两个`agent`
  * `A`利用自身行为与`B`的行为完成拟合
  * `B`在`A`获胜时替换为`A`的参数
  * `A`预期是在前期多随机学习更多的行为,后期主要靠自身参数引导
  * `B`预期是在训练的全阶段都存在一定概率的随机,使得训练中持续存在更多的行为